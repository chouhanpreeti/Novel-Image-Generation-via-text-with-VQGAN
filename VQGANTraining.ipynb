{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxbKPn--3Jos","executionInfo":{"status":"ok","timestamp":1681064278002,"user_tz":240,"elapsed":21599,"user":{"displayName":"Pia","userId":"11289152077416308748"}},"outputId":"543ebd02-cb48-49d2-fb8e-fc22ca6dcbcc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","import os\n","model_folder = '/content/drive/MyDrive/GM Project/VQGAN Project/model'\n","helper_methods_folder = '/content/drive/MyDrive/GM Project/VQGAN Project/utils'\n","\n","sys.path.append(os.path.abspath(model_folder))\n","sys.path.append(os.path.abspath(helper_methods_folder))"],"metadata":{"id":"Us1sMcCe3e5T","executionInfo":{"status":"ok","timestamp":1681064279437,"user_tz":240,"elapsed":11,"user":{"displayName":"Pia","userId":"11289152077416308748"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import os\n","import argparse\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torchvision import utils as vutils\n","from discriminator import Discriminator\n","from perceptualloss import LPIPS\n","from vqgan import VQGAN\n","from utils import load_data, weights_init"],"metadata":{"id":"gts_-Hdc3w-P","executionInfo":{"status":"ok","timestamp":1681064293211,"user_tz":240,"elapsed":13782,"user":{"displayName":"Pia","userId":"11289152077416308748"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def configure_optimizers():\n","    lr = 2.25e-05\n","    opt_vq = torch.optim.Adam(\n","        list(vqgan.encoder.parameters()) +\n","        list(vqgan.decoder.parameters()) +\n","        list(vqgan.codebook.parameters()) +\n","        list(vqgan.quant_conv.parameters()) +\n","        list(vqgan.post_quant_conv.parameters()),\n","        lr=lr, eps=1e-08, betas=(0.5, 0.9)\n","    )\n","    opt_disc = torch.optim.Adam(discriminator.parameters(),\n","                                lr=lr, eps=1e-08, betas=(0.5, 0.9))\n","\n","    return opt_vq, opt_disc"],"metadata":{"id":"nKxH3R7p5nN5","executionInfo":{"status":"ok","timestamp":1681064293579,"user_tz":240,"elapsed":372,"user":{"displayName":"Pia","userId":"11289152077416308748"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["vqgan = VQGAN().to(device=\"cuda:0\")\n","discriminator = Discriminator().to(device=\"cuda:0\")\n","discriminator.apply(weights_init)\n","perceptual_loss = LPIPS().eval().to(device=\"cuda:0\")\n","opt_vq, opt_disc = configure_optimizers()"],"metadata":{"id":"Wkfxv2_n5rz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = load_data(dataset_path=r\"/content/drive/MyDrive/GM Project/coco2017\")\n","steps_per_epoch = len(train_dataset)\n","for epoch in range(10):\n","    with tqdm(range(len(train_dataset))) as pbar:\n","        for i, imgs in zip(pbar, train_dataset):\n","            imgs = imgs.to(device=\"cuda:0\")\n","            decoded_images, _, q_loss = vqgan(imgs)\n","\n","            disc_real = discriminator(imgs)\n","            disc_fake = discriminator(decoded_images)\n","\n","            disc_factor = vqgan.adopt_weight(1., epoch * steps_per_epoch + i, threshold=10000)\n","\n","            _perceptual_loss = perceptual_loss(imgs, decoded_images)\n","            rec_loss = torch.abs(imgs - decoded_images)\n","            perceptual_rec_loss = 1. * _perceptual_loss + 1. * rec_loss\n","            perceptual_rec_loss = perceptual_rec_loss.mean()\n","            g_loss = -torch.mean(disc_fake)\n","\n","            λ = vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n","            vq_loss = perceptual_rec_loss + q_loss + disc_factor * λ * g_loss\n","\n","            d_loss_real = torch.mean(F.relu(1. - disc_real))\n","            d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n","            gan_loss = disc_factor * 0.5 * (d_loss_real + d_loss_fake)\n","\n","            opt_vq.zero_grad()\n","            vq_loss.backward(retain_graph=True)\n","\n","            opt_disc.zero_grad()\n","            gan_loss.backward()\n","\n","            opt_vq.step()\n","            opt_disc.step()\n","\n","            if i % 100 == 0:\n","                with torch.no_grad():\n","                    real_fake_images = torch.cat((imgs.add(1).mul(0.5)[:4], decoded_images.add(1).mul(0.5)[:4]))\n","                    vutils.save_image(real_fake_images, os.path.join(\"/content/drive/MyDrive/GM Project/results/\", f\"{epoch}_{i}.jpg\"), nrow=4)\n","\n","            pbar.set_postfix(\n","                VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n","                GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n","            )\n","            pbar.update(0)\n","\n","            torch.save(vqgan.state_dict(), os.path.join(\"/content/drive/MyDrive/GM Project/checkpoints/\", f\"vqgan_epoch_{epoch}.pt\"))\n","                \n","torch.save(vqgan.state_dict(), os.path.join(\"/content/drive/MyDrive/GM Project/checkpoints/\", f\"vqgan_final_.pt\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xtCfMwdD6RHT","outputId":"5ae65164-abe9-466f-800a-fa368eb24630"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1637/1637 [3:22:34<00:00,  7.42s/it, GAN_Loss=0, VQ_Loss=1.26]\n","100%|██████████| 1637/1637 [1:26:09<00:00,  3.16s/it, GAN_Loss=0, VQ_Loss=0.473]\n"," 52%|█████▏    | 856/1637 [45:03<39:53,  3.07s/it, GAN_Loss=0, VQ_Loss=0.369]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"r4wL9rPus2ny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"73kln4znbyTM"},"execution_count":null,"outputs":[]}]}