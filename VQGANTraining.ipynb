{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPODDZsPrB1FJFps7QrDenG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxbKPn--3Jos","executionInfo":{"status":"ok","timestamp":1681043101406,"user_tz":240,"elapsed":28330,"user":{"displayName":"Preeti Chouhan","userId":"18231851292304029863"}},"outputId":"979ecf1c-a0fb-42a0-8397-d5f098a7995b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","import os\n","model_folder = '/content/drive/MyDrive/GM Project/VQGAN Project/model'\n","helper_methods_folder = '/content/drive/MyDrive/GM Project/VQGAN Project/utils'\n","\n","sys.path.append(os.path.abspath(model_folder))\n","sys.path.append(os.path.abspath(helper_methods_folder))"],"metadata":{"id":"Us1sMcCe3e5T","executionInfo":{"status":"ok","timestamp":1681043101407,"user_tz":240,"elapsed":12,"user":{"displayName":"Preeti Chouhan","userId":"18231851292304029863"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import os\n","import argparse\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torchvision import utils as vutils\n","from discriminator import Discriminator\n","from perceptualloss import LPIPS\n","from vqgan import VQGAN\n","from utils import load_data, weights_init"],"metadata":{"id":"gts_-Hdc3w-P","executionInfo":{"status":"ok","timestamp":1681043115871,"user_tz":240,"elapsed":14474,"user":{"displayName":"Preeti Chouhan","userId":"18231851292304029863"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def configure_optimizers():\n","    lr = 2.25e-05\n","    opt_vq = torch.optim.Adam(\n","        list(vqgan.encoder.parameters()) +\n","        list(vqgan.decoder.parameters()) +\n","        list(vqgan.codebook.parameters()) +\n","        list(vqgan.quant_conv.parameters()) +\n","        list(vqgan.post_quant_conv.parameters()),\n","        lr=lr, eps=1e-08, betas=(0.5, 0.9)\n","    )\n","    opt_disc = torch.optim.Adam(discriminator.parameters(),\n","                                lr=lr, eps=1e-08, betas=(0.5, 0.9))\n","\n","    return opt_vq, opt_disc"],"metadata":{"id":"nKxH3R7p5nN5","executionInfo":{"status":"ok","timestamp":1681043115872,"user_tz":240,"elapsed":5,"user":{"displayName":"Preeti Chouhan","userId":"18231851292304029863"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["vqgan = VQGAN().to(device=\"cuda:0\")\n","discriminator = Discriminator().to(device=\"cuda:0\")\n","discriminator.apply(weights_init)\n","perceptual_loss = LPIPS().eval().to(device=\"cuda:0\")\n","opt_vq, opt_disc = configure_optimizers()"],"metadata":{"id":"Wkfxv2_n5rz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = load_data(dataset_path=r\"/content/drive/MyDrive/GM Project/coco2017\")\n","steps_per_epoch = len(train_dataset)\n","for epoch in range(1):\n","    with tqdm(range(len(train_dataset))) as pbar:\n","        for i, imgs in zip(pbar, train_dataset):\n","            imgs = imgs.to(device=\"cuda:0\")\n","            decoded_images, _, q_loss = vqgan(imgs)\n","\n","            disc_real = discriminator(imgs)\n","            disc_fake = discriminator(decoded_images)\n","\n","            disc_factor = vqgan.adopt_weight(1., epoch * steps_per_epoch + i, threshold=10000)\n","\n","            _perceptual_loss = perceptual_loss(imgs, decoded_images)\n","            rec_loss = torch.abs(imgs - decoded_images)\n","            perceptual_rec_loss = 1. * _perceptual_loss + 1. * rec_loss\n","            perceptual_rec_loss = perceptual_rec_loss.mean()\n","            g_loss = -torch.mean(disc_fake)\n","\n","            λ = vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n","            vq_loss = perceptual_rec_loss + q_loss + disc_factor * λ * g_loss\n","\n","            d_loss_real = torch.mean(F.relu(1. - disc_real))\n","            d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n","            gan_loss = disc_factor * 0.5 * (d_loss_real + d_loss_fake)\n","\n","            opt_vq.zero_grad()\n","            vq_loss.backward(retain_graph=True)\n","\n","            opt_disc.zero_grad()\n","            gan_loss.backward()\n","\n","            opt_vq.step()\n","            opt_disc.step()\n","\n","            if i % 100 == 0:\n","                with torch.no_grad():\n","                    real_fake_images = torch.cat((imgs.add(1).mul(0.5)[:4], decoded_images.add(1).mul(0.5)[:4]))\n","                    vutils.save_image(real_fake_images, os.path.join(\"/content/drive/MyDrive/GM Project/results/\", f\"{epoch}_{i}.jpg\"), nrow=4)\n","\n","            pbar.set_postfix(\n","                VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n","                GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n","            )\n","            pbar.update(0)\n","\n","            if (epoch % 10) == 0:\n","                torch.save(vqgan.state_dict(), os.path.join(\"/content/drive/MyDrive/GM Project/checkpoints/\", f\"vqgan_epoch_{epoch}.pt\"))\n","                \n","torch.save(vqgan.state_dict(), os.path.join(\"/content/drive/MyDrive/GM Project/checkpoints/\", f\"vqgan_final_.pt\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xtCfMwdD6RHT","outputId":"f80b2070-f313-4366-d453-eac75ab95f16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 11%|█         | 172/1637 [23:02<3:07:54,  7.70s/it, GAN_Loss=0, VQ_Loss=12.9]"]}]}]}