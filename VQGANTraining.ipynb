{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 11.1\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1681064279437,
     "user": {
      "displayName": "Pia",
      "userId": "11289152077416308748"
     },
     "user_tz": 240
    },
    "id": "Us1sMcCe3e5T"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.discriminator import Discriminator\n",
    "from model.perceptualloss import LPIPS\n",
    "from model.vqgan import VQGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.utils import load_data, weights_init\n",
    "\n",
    "class ImagePaths(Dataset):\n",
    "    def __init__(self, path, size=None):\n",
    "        self.size = size\n",
    "\n",
    "        self.images = [os.path.join(path, file) for file in os.listdir(path)]\n",
    "        self._length = len(self.images)\n",
    "\n",
    "        self.rescaler = albumentations.SmallestMaxSize(max_size=self.size)\n",
    "        self.cropper = albumentations.CenterCrop(height=self.size, width=self.size)\n",
    "        self.preprocessor = albumentations.Compose([self.rescaler, self.cropper])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    # image normalization\n",
    "    def preprocess_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = self.preprocessor(image=image)[\"image\"]\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = self.preprocess_image(self.images[i])\n",
    "        return example\n",
    "\n",
    "\n",
    "# load the data\n",
    "def load_data(dataset_path):\n",
    "    train_data = ImagePaths(dataset_path, size=256)\n",
    "    train_loader = DataLoader(train_data, batch_size=2, shuffle=False)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "# Module Utils for Encoder, Decoder etc.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "def plot_images(images):\n",
    "    x = images[\"input\"]\n",
    "    reconstruction = images[\"rec\"]\n",
    "    half_sample = images[\"half_sample\"]\n",
    "    full_sample = images[\"full_sample\"]\n",
    "\n",
    "    fig, axarr = plt.subplots(1, 4)\n",
    "    axarr[0].imshow(x.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
    "    axarr[1].imshow(reconstruction.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
    "    axarr[2].imshow(half_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
    "    axarr[3].imshow(full_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 13782,
     "status": "ok",
     "timestamp": 1681064293211,
     "user": {
      "displayName": "Pia",
      "userId": "11289152077416308748"
     },
     "user_tz": 240
    },
    "id": "gts_-Hdc3w-P"
   },
   "source": [
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils as vutils\n",
    "from discriminator import Discriminator\n",
    "from perceptualloss import LPIPS\n",
    "from vqgan import VQGAN\n",
    "from utils import load_data, weights_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1681064293579,
     "user": {
      "displayName": "Pia",
      "userId": "11289152077416308748"
     },
     "user_tz": 240
    },
    "id": "nKxH3R7p5nN5"
   },
   "outputs": [],
   "source": [
    "def configure_optimizers():\n",
    "    lr = 2.25e-05\n",
    "    \n",
    "    opt_vq = torch.optim.Adam(\n",
    "        list(vqgan.encoder.parameters()) +\n",
    "        list(vqgan.decoder.parameters()) +\n",
    "        list(vqgan.codebook.parameters()) +\n",
    "        list(vqgan.quant_conv.parameters()) +\n",
    "        list(vqgan.post_quant_conv.parameters()),\n",
    "        lr=lr, eps=1e-08, betas=(0.5, 0.9)\n",
    "    )\n",
    "    \n",
    "    opt_disc = torch.optim.Adam(discriminator.parameters(),\n",
    "                                lr=lr, eps=1e-08, betas=(0.5, 0.9))\n",
    "\n",
    "    return opt_vq, opt_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Wkfxv2_n5rz0"
   },
   "outputs": [],
   "source": [
    "vqgan = VQGAN().to(device=\"cuda:0\")\n",
    "discriminator = Discriminator().to(device=\"cuda:0\")\n",
    "discriminator.apply(weights_init)\n",
    "perceptual_loss = LPIPS().eval().to(device=\"cuda:0\")\n",
    "opt_vq, opt_disc = configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtCfMwdD6RHT",
    "outputId": "5ae65164-abe9-466f-800a-fa368eb24630"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4910/4910 [1:19:45<00:00,  1.03it/s, GAN_Loss=0, VQ_Loss=0.254] \n",
      "100%|██████████| 4910/4910 [35:38<00:00,  2.30it/s, GAN_Loss=0, VQ_Loss=0.147] \n",
      "100%|██████████| 4910/4910 [1:19:43<00:00,  1.03it/s, GAN_Loss=0, VQ_Loss=0.175]     \n",
      "100%|██████████| 4910/4910 [35:37<00:00,  2.30it/s, GAN_Loss=0, VQ_Loss=0.153]     \n",
      "100%|██████████| 4910/4910 [1:20:01<00:00,  1.02it/s, GAN_Loss=0, VQ_Loss=0.162]     \n",
      "100%|██████████| 4910/4910 [35:26<00:00,  2.31it/s, GAN_Loss=0.004, VQ_Loss=0.154] \n",
      "100%|██████████| 4910/4910 [1:18:05<00:00,  1.05it/s, GAN_Loss=0, VQ_Loss=0.173]     \n",
      "100%|██████████| 4910/4910 [35:26<00:00,  2.31it/s, GAN_Loss=0, VQ_Loss=0.132]     \n",
      "100%|██████████| 4910/4910 [1:20:02<00:00,  1.02it/s, GAN_Loss=0.001, VQ_Loss=0.118] \n",
      "100%|██████████| 4910/4910 [35:32<00:00,  2.30it/s, GAN_Loss=0, VQ_Loss=0.143]     \n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_data(dataset_path=r\"C:\\Users\\P_CHOUH\\Documents\\GMProject\\coco2017\")\n",
    "steps_per_epoch = len(train_dataset)\n",
    "for epoch in range(10):\n",
    "    with tqdm(range(len(train_dataset))) as pbar:\n",
    "        for i, imgs in zip(pbar, train_dataset):\n",
    "            imgs = imgs.to(device=\"cuda:0\")\n",
    "            decoded_images, _, q_loss = vqgan(imgs)\n",
    "\n",
    "            disc_real = discriminator(imgs)\n",
    "            disc_fake = discriminator(decoded_images)\n",
    "\n",
    "            disc_factor = vqgan.adopt_weight(1., epoch * steps_per_epoch + i, threshold=10000)\n",
    "\n",
    "            _perceptual_loss = perceptual_loss(imgs, decoded_images)\n",
    "            rec_loss = torch.abs(imgs - decoded_images)\n",
    "            perceptual_rec_loss = 1. * _perceptual_loss + 1. * rec_loss\n",
    "            perceptual_rec_loss = perceptual_rec_loss.mean()\n",
    "            g_loss = -torch.mean(disc_fake)\n",
    "\n",
    "            λ = vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n",
    "            vq_loss = perceptual_rec_loss + q_loss + disc_factor * λ * g_loss\n",
    "\n",
    "            d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
    "            d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
    "            gan_loss = disc_factor * 0.5 * (d_loss_real + d_loss_fake)\n",
    "\n",
    "            opt_vq.zero_grad()\n",
    "            vq_loss.backward(retain_graph=True)\n",
    "\n",
    "            opt_disc.zero_grad()\n",
    "            gan_loss.backward()\n",
    "\n",
    "            opt_vq.step()\n",
    "            opt_disc.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    real_fake_images = torch.cat((imgs.add(1).mul(0.5)[:4], decoded_images.add(1).mul(0.5)[:4]))\n",
    "                    vutils.save_image(real_fake_images, os.path.join(r\"C:/Users/P_CHOUH/Documents/GMProject/results/\", f\"{epoch}_{i}.jpg\"), nrow=4)\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n",
    "                GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n",
    "            )\n",
    "            pbar.update(0)\n",
    "\n",
    "            if (epoch % 2) == 0:\n",
    "                torch.save(vqgan.state_dict(), os.path.join(r\"C:/Users/P_CHOUH/Documents/GMProject/checkpoints/\", f\"vqgan_epoch_{epoch}.pt\"))\n",
    "                \n",
    "torch.save(vqgan.state_dict(), os.path.join(r\"C:/Users/P_CHOUH/Documents/GMProject/checkpoints\", f\"vqgan_final_.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4wL9rPus2ny"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73kln4znbyTM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
